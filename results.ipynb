{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration and prediction + plots for journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob, os\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import Counter\n",
    "import forestci as fci\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline\n",
    "import scipy.stats as st\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "'''Relevant paths'''\n",
    "path_models = [] # trained models path\n",
    "path = [] # data path\n",
    "path_to_save_figures = [] # path to save results to \n",
    "\n",
    "# import functions\n",
    "from results_plotting_functions import rmspe, alpha_accuracy_beta_rlh, count_entries_per_interval, predict_prob, std_calibrated, calibrated_prob, calibration_isotonic_regression_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select datsets as per Nature NMI - see table in papper for refrence:\n",
    "1. Group I: full_dataset \n",
    "2. Group II: standford_dataset\n",
    "3.  Group III: oxford_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select battery \n",
    "battery =  # group_1, group_2, group_3\n",
    "adversarial = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_hat, y_test, sem, sem_calibrated, z_value):\n",
    "    # plot predicted results before and after re-calibration\n",
    "    '''plotting the confidence interval and the prediction results'''\n",
    "    x_axis = list(X_test.index.values)\n",
    "    fig1 = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # confidence interval\n",
    "    \n",
    "    # plt.errorbar(X_test.Cycle, y_hat, yerr=2 * sem, fmt='o', capsize=3, linewidth=2, ms=2)\n",
    "    y_up = y_hat + z_value * sem\n",
    "    y_low = y_hat - z_value * sem\n",
    "    \n",
    "    y_up_calibrated = y_hat + z_value * sem_calibrated\n",
    "    y_low_calibrated = y_hat - z_value * sem_calibrated\n",
    "    \n",
    "    # accuracy zone calculationa\n",
    "    y_acc_zone_up = y_test+alpha_acc_zone*y_test\n",
    "    y_acc_zone_low = y_test-alpha_acc_zone*y_test\n",
    "\n",
    "\n",
    "    #calculate predictions in accuracy zone\n",
    "    acc_zone_entries, beta, rlh = alpha_accuracy_beta_rlh(y_test, y_hat, sem_calibrated, alpha_acc_zone)\n",
    "    \n",
    "    \n",
    "#     plt.fill_between(x_axis, y_low, y_up, color='grey', alpha=0.4)\n",
    "    plt.plot(x_axis, y_test, 'ro', markersize=4,  alpha=.6, label='True capacity')\n",
    "#     plt.fill_between(x_axis, y_acc_zone_low, y_acc_zone_up, color='blue', alpha=0.4, label='Accuracy zone [-$\\\\alpha$, $\\\\alpha$]')\n",
    "    \n",
    "    plt.plot(x_axis, y_hat, 'o', color='k', markersize=4, alpha=.6, label='Predicted capacity')\n",
    "    plt.fill_between(x_axis, y_low_calibrated, y_up_calibrated, color='orange', alpha=0.4, label='±2$\\sigma$')\n",
    "\n",
    "#     plt.errorbar(x_axis, y_hat, yerr=np.sqrt(V_IJ_unbiased), fmt='o')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.xlabel('Cycle', fontsize=20)\n",
    "    if battery == 'oxford_dataset':\n",
    "        plt.ylabel('Capacity [mAh]', fontsize=20)\n",
    "    else:\n",
    "        plt.ylabel('Capacity [Ah]', fontsize=20)\n",
    "\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "\n",
    "#     plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # calculate accuracy \n",
    "    \n",
    "    r2 = r2_score(y_test, y_hat)\n",
    "    mape = np.round(np.mean(np.abs((y_test - y_hat) / y_test))*100,2)\n",
    "    rmspe_calc = np.round(np.mean(np.sqrt(np.mean(np.square(((y_test - y_hat) / y_test)))))*100,2)\n",
    "    mse = mean_squared_error(y_hat, y_test)\n",
    "    \n",
    "    \n",
    "    print('Accuracy based on $R^2$: {}'.format(r2))\n",
    "    print('Accuracy based on Mean Absolute Percentage Error: {}'.format(mape))\n",
    "    print('Accuracy based on Root Mean Squared Percentage Error: {}'.format(rmspe_calc))\n",
    "    print('Mean Square Error: {}'.format(mse))\n",
    "    print('% of entriers falling in the accuracy zone: {}'.format(acc_zone_entries)) # note this is percentage\n",
    "    print('Average probability mass of the prediciton PDF within the accuracy zone: {}'.format(beta))\n",
    "    print('Ratio of early to late predictions when references to accurazy zone boundries: {}'.format(rlh))\n",
    "\n",
    "    '''Calculate the calibrations score\n",
    "       reference: Gaussian process regression for In-situ Capacity Estimation of Lithium-ion Batteries\n",
    "       Rober R. Richardson, Birkl, Howey\n",
    "       '''    \n",
    "    CS = 1/len(X_test) * np.sum((np.abs(y_hat - y_test) < Z * sem))\n",
    "    CS_calibrated = 1/len(X_test) * np.sum((abs(y_hat - y_test) < Z * sem_calibrated))\n",
    "    SH = np.mean(sem_calibrated)\n",
    "    \n",
    "    print('Calibration score: {}'.format(CS))\n",
    "    print('Calibration score calibrated model: {}'.format(CS_calibrated))\n",
    "    print('Sharpness (mean std): {}'.format(SH))\n",
    "    \n",
    "    ''' Print the two capacities on the same plot without the cycle number'''\n",
    "    fig2 = plt.figure(figsize=(10, 10))\n",
    "    # acc zone display uniformly\n",
    "    y = np.linspace(np.min(y_test), np.max(y_test), 100)\n",
    "    y_up = y + alpha_acc_zone*y\n",
    "    y_low = y - alpha_acc_zone*y\n",
    "    ax1 = plt.axes()  # standard axes\n",
    "    ax1.plot(y_test, y_test, 'ro', alpha=.7, zorder=10, label = 'True Capacity ($y^*$)', markersize=2)\n",
    "    ax1.fill_between(y, y_low, y_up, color='green', alpha=0.6, zorder=10, label='Accuracy zone [-$\\\\alpha$, $\\\\alpha$]')\n",
    "#     ax1.plot(y_test, y_hat, 'o', color='k', alpha=0.6, zorder=10, label='Predicted Capacity')\n",
    "#     ax1.errorbar(y_test, y_hat, yerr=Z*sem, fmt='--o', color='blue', alpha=0.4, zorder=5, )\n",
    "    markers, caps, bars = ax1.errorbar(y_test, y_hat, yerr=Z*sem_calibrated, ecolor='orange',capsize=2, capthick=2,\n",
    "                                       fmt='--o', color='black',alpha=0.6, zorder=5, label='Predicted capacity ($\\hat{y}^*$) ±2$\\sigma$')\n",
    "\n",
    "    # loop through bars and caps and set the alpha value\n",
    "    [bar.set_alpha(0.4) for bar in bars]\n",
    "    [cap.set_alpha(0.4) for cap in caps]\n",
    "    ax1.invert_xaxis()\n",
    "\n",
    "#     plt.plot(y_test, y_hat, 'o', color='orange', alpha=0.6)\n",
    "#     plt.gca().invert_xaxis()\n",
    "    plt.xlabel('True Capacity [Ah]', fontsize=20)\n",
    "    if battery == 'oxford_dataset':\n",
    "        plt.ylabel('Capacity [mAh]', fontsize=20)\n",
    "    else:\n",
    "        plt.ylabel('Capacity [Ah]', fontsize=20)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "#     plt.title('True vs. Predicted Capacity', fontsize=15)\n",
    "#     plt.grid(True)\n",
    "    ax2 = plt.axes([0.21, 0.21, 0.24, 0.24])\n",
    "    percent_error = (y_hat-y_test)/y_test\n",
    "    if battery in ['full_dataset', 'standford_dataset']:\n",
    "        no_of_bins = 50 # 50-full_datset, 10 all other datasets\n",
    "    else:\n",
    "        no_of_bins = 10 # 50-full_datset, 10 all other datasets\n",
    "    n, bins, patches = ax2.hist(percent_error, no_of_bins, edgecolor='white', color='black')\n",
    "    \n",
    "    for c, p in zip(bins, patches):\n",
    "        if c >= alpha_acc_zone:\n",
    "            plt.setp(p, 'facecolor', 'black')\n",
    "        elif c <= -alpha_acc_zone:\n",
    "            plt.setp(p, 'facecolor', 'black')\n",
    "        else:\n",
    "            plt.setp(p, 'facecolor', 'green') #mediumslateblue\n",
    "    plt.axvline(0, color='red', linestyle='dashed')#, linewidth=1)\n",
    "    plt.axvline(-alpha_acc_zone, color='green', linestyle='dashed')#, linewidth=1)\n",
    "    plt.axvline(alpha_acc_zone, color='green', linestyle='dashed')#, linewidth=1)\n",
    "    plt.axvline(0, color='red', linestyle='dashed')#, linewidth=1)\n",
    "    plt.xlabel('% error: $(\\hat{y}^*-y^*)/y^*$', fontsize=15)\n",
    "    plt.ylabel('No. of entries', fontsize=15)\n",
    "    plt.title('Histogram of % error wrt $\\hat{y}^*$', fontsize=15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "#     plt.xlim([-1.2*np.max(abs(y_test-y_hat)), 1.2*np.max(abs(y_test-y_hat))])\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(abs(y_test-y_hat), sem_calibrated, '*')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.xlabel('Error')\n",
    "\n",
    "#     ax2.text(0.1, .5, r'$-\\alpha$')\n",
    "    \n",
    "    return fig1, fig2, r2, mape, rmspe_calc, mse, acc_zone_entries, beta, rlh, CS_calibrated, SH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test data\n",
    "# data_test = pd.read_csv(path + 'data_test_fsed__adversarial' + battery +'.csv', index_col=0)\n",
    "data_test = pd.read_csv()\n",
    "X_test_complete = data_test.drop(['Discharge_Q', 'Group'], axis=1)\n",
    "y_test_complete = data_test.Discharge_Q\n",
    "data_test_4_group = data_test.Group\n",
    "\n",
    "# load relevant battery dataset for training the algorithm\n",
    "if adversarial:\n",
    "    data_train = pd.read_csv()\n",
    "else:\n",
    "    data_train = pd.read_csv()\n",
    "    \n",
    "# train data used to train the model\n",
    "# data_train = pd.read_csv(path + 'data_train_fsed_'+ battery + '.csv', index_col=0)\n",
    "X_train = data_train.drop(['Discharge_Q', 'Group'], axis=1)\n",
    "y_train = data_train.Discharge_Q\n",
    "\n",
    "# re-calibration data\n",
    "\n",
    "\n",
    "data_calibration = pd.read_csv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of features selected based on RFE-CV: {} for battery dataset {}'.format(len(list(X_train)), battery))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use +/- alpha accuracy zone as per reference: \"On Applying the Prognostics Performance Metrics by Saxena NASA\"\n",
    "'''\n",
    "alpha_acc_zone = 0.015 # how to determine the alpha value - 1.5% is the arbina error, but how to justify the rest?\n",
    "Z = st.norm.ppf(1-(1-0.9)/2) # calculate z-value for 90% confidence\n",
    "STD = alpha_acc_zone/Z\n",
    "var_expected = STD**2\n",
    "print('Expected variance based on domain specific selection: {}'.format(var_expected))\n",
    "print('Z-value used: {}'.format(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load models'''\n",
    "# random forest model\n",
    "RF_model_filenname = path_models + 'RF_model_adversarial_'+ battery +'.sav'\n",
    "# RF_model_filenname = path_models + 'RF_model_'+ battery +'.sav'\n",
    "RF = pickle.load(open(RF_model_filenname, 'rb'))\n",
    "RF_model = RF.best_estimator_\n",
    "\n",
    "# bayesian ridge \n",
    "Bayes_Ridge_filename = path_models + 'Bayesian_Ridge_adversarial_'+ battery +'.sav'\n",
    "# Bayes_Ridge_filename = path_models + 'Bayesian_Ridge_'+ battery +'.sav'\n",
    "Bayes_Ridge = pickle.load(open(Bayes_Ridge_filename, 'rb'))\n",
    "Bayes_Ridge_model = Bayes_Ridge.best_estimator_\n",
    "\n",
    "# Gaussian process model\n",
    "GPR_model_filenname = path_models + 'GPR_model_'+ battery +'.sav'\n",
    "GPR = pickle.load(open(GPR_model_filenname, 'rb'))\n",
    "GPR_model = GPR.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with a selected model - HARD CODED\n",
    "\n",
    "model_name = 'GPR_model' # RF_model, Bayes_Ridge_model, GPR_model\n",
    "model = GPR_model # RF_model, Bayes_Ridge_model, GPR_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_calibration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r2_vec = []\n",
    "mape_vec = []\n",
    "rmspe_vec = []\n",
    "mse_vec = []\n",
    "acc_zone_percentage_vec = []\n",
    "beta_vec = []\n",
    "rlh_vec = []\n",
    "avg_calibration_vec = []\n",
    "SH_vec = []\n",
    "\n",
    "for i in data_test_4_group.unique():\n",
    "    \n",
    "    group_calibration = [] # specify the group of cells on which to calibrate the model \n",
    "\n",
    "    # load calibration data per group\n",
    "    data_calibration_group = data_calibration[data_calibration.Group.isin(group_calibration)]\n",
    "\n",
    "    X_calibration = data_calibration_group.drop(['Discharge_Q', 'Group'], axis=1)\n",
    "    y_calibration = data_calibration_group.Discharge_Q\n",
    "\n",
    "    calibration_model = calibration_isotonic_regression_model(model_name, model, X_calibration, y_calibration, X_train)\n",
    "    print('Calibration done!')\n",
    "\n",
    "\n",
    "    data_group = data_test_4_group[data_test_4_group==i]\n",
    "    min_group = data_group.index[0]\n",
    "    max_group = data_group.index[-1]\n",
    "    \n",
    "    X_test = X_test_complete[min_group:max_group]\n",
    "    X_test.reset_index(inplace=True, drop=True)\n",
    "    y_test = y_test_complete[min_group:max_group]\n",
    "    y_test.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    if model_name in ['Bayes_Ridge_model', 'GPR_model']:\n",
    "        y_hat,  sem = model.predict(X_test, return_std=True)\n",
    "        \n",
    "    elif model_name == 'RF_model':\n",
    "        y_hat = model.predict(X_test)\n",
    "        sem = np.sqrt(fci.random_forest_error(RF_model, X_train, X_test))\n",
    "\n",
    "    else:\n",
    "        y_hat,  sem = model.predict(X_test)\n",
    "\n",
    "    prob_per_int_y_test, prob_y_test, prob_y_test_expected, prob = count_entries_per_interval(y_test, y_hat, sem)\n",
    "    prob_model_y_test = predict_prob(y_test, y_hat, sem)\n",
    "    \n",
    "    prob_test_calibrated = calibrated_prob(calibration_model, prob_model_y_test)\n",
    "\n",
    "    sem_calibrated = std_calibrated(y_test, y_hat, prob_test_calibrated)\n",
    "    prob_per_int_y_test_calibrated, prob_y_test_calibrated, prob_y_test_expected, prob = count_entries_per_interval(y_test, y_hat, sem_calibrated)\n",
    "\n",
    "    '''Plotting calibration only'''\n",
    "    \n",
    "    # prob_calibration_calibrated = ir.transform(prob_model_y_calibration)\n",
    "    plt.figure()\n",
    "#     plt.title('Test data')\n",
    "    plt.plot(prob, prob, 'k--')\n",
    "    plt.plot(prob, prob_per_int_y_test, 'b*--')\n",
    "    plt.plot(prob, prob_per_int_y_test_calibrated, '*--', c='orange')\n",
    "    plt.xlabel('Expected Cofidence Level', fontsize=20)\n",
    "    plt.ylabel('Observed Confidence Level', fontsize=20)\n",
    "    plt.legend(['Ideal calibration', 'Uncalibrated', 'Calibrated'], fontsize=20)\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    print('------------------------Battery group: {}'.format(i))\n",
    "    # function to plot the results\n",
    "    fig1, fig2, r2, mape, rmspe, mse, acc_zone_percentage, beta, rlh, CS, SH = plot_results(y_hat, y_test, sem, sem_calibrated, Z)\n",
    "       \n",
    "   # plot reliability curve for the paper\n",
    "    if r2 < 0 or len(prob)<11: # plot reliability curves for an r2 better greate than 0\n",
    "        pass\n",
    "    else:\n",
    "        # note: if there is not suffiecint data in index prob rounding should go to 1\n",
    "        index = []\n",
    "        round_value = 2   # <---- for more granularity also changethe round value\n",
    "        for q in np.round(np.linspace(0, 1, 11), round_value):\n",
    "            \n",
    "#                                                                      | CHANGE THIS LOCATION TO OBTAIN A MORE ACCURATE CALIBRATION CURVE FOR  THE PLOTS\n",
    "            index.append(np.where(np.round(prob, round_value) == q)[0][2]) # was [0][1] i.e. checking second element, used to raound ro 2\n",
    "                                                                    #[0][2]\n",
    "        from operator import itemgetter \n",
    "        prob_expected = itemgetter(*index)(prob)\n",
    "        prob_observed_uncalibrated = itemgetter(*index)(prob_per_int_y_test)\n",
    "        prob_observed_calibrated = itemgetter(*index)(prob_per_int_y_test_calibrated)\n",
    "        fig3 = plt.figure(figsize=(10,10))\n",
    "    #     plt.title('Test data')\n",
    "        plt.plot(prob_expected, prob_expected, 'k--', markersize=2)\n",
    "        plt.plot(prob_expected, prob_observed_uncalibrated, 'bo--', markersize=10, linewidth=6)\n",
    "        plt.plot(prob_expected, prob_observed_calibrated, 'o--', c='orange', markersize=10, linewidth=6)\n",
    "        plt.xlabel('Expected Cofidence Level', fontsize=20)\n",
    "        plt.ylabel('Observed Confidence Level', fontsize=20)\n",
    "        plt.legend(['Ideal calibration', 'Uncalibrated', 'Calibrated'], fontsize=20)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    #     plt.grid(True) \n",
    "\n",
    "#     '''Save figures'''    \n",
    "    print('saved askjahsdkjhsadkashdhasjh')\n",
    "    fig3.savefig(path_to_save_figures + \"cell_no_\" + str(int(i))+ '_' + model_name +  \".pdf\", bbox_inches='tight')\n",
    "    fig1.savefig(path_to_save_figures + \"cell_no_\"+ str(int(i))+ '_' + model_name + \".pdf\", bbox_inches='tight')\n",
    "    fig2.savefig(path_to_save_figures + \"cell_no_\" + str(int(i))+ '_' + model_name + \".pdf\", bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    # average results\n",
    "    cycles_w_neg_r2 = []\n",
    "    if r2 > 0:\n",
    "        cycles_w_neg_r2.append(i)\n",
    "        \n",
    "    r2_vec.append(r2)\n",
    "    mape_vec.append(mape)\n",
    "    rmspe_vec.append(rmspe)\n",
    "    mse_vec.append(mse)\n",
    "    acc_zone_percentage_vec.append(acc_zone_percentage)\n",
    "    beta_vec.append(beta)\n",
    "    rlh_vec.append(rlh)\n",
    "    avg_calibration_vec.append(CS)\n",
    "    SH_vec.append(SH)\n",
    "\n",
    "# avergae values across dataset\n",
    "r2_mean = np.mean(r2_vec)\n",
    "mape_mean = np.mean(mape_vec)\n",
    "rmspe_mean = np.mean(rmspe_vec)\n",
    "mse_mean = np.mean(mse_vec)\n",
    "acc_zone_percentage_mean = np.mean(acc_zone_percentage_vec)\n",
    "beta_mean = np.mean(beta_vec)\n",
    "rlh_mean = np.mean(rlh_vec)\n",
    "calibration_mean = np.mean(avg_calibration_vec)\n",
    "SH_mean = np.mean(SH_vec)\n",
    "\n",
    "print('#############################################################################################')\n",
    "print('Mean accuracy based on $R^2$: {}'.format(r2_mean))\n",
    "print('Mean accuracy based on Mean Absolute Percentage Error: {}'.format(mape_mean))\n",
    "print('Mean accuracy based on Root Mean Squared Percentage Error: {}'.format(rmspe_mean))\n",
    "print('Mean Mean Square Error: {}'.format(mse_mean))\n",
    "print('Mean % of entriers falling in the accuracy zone: {}'.format(acc_zone_percentage_mean)) # note this is percentage\n",
    "print('Mean average probability mass of the prediciton PDF within the accuracy zone: {}'.format(beta_mean))\n",
    "print('Mean percentage of early predictions when references to accurazy zone boundries: {}'.format(rlh_mean))\n",
    "print('Mean calibration score calibrated model: {}'.format(calibration_mean))\n",
    "print('Mean sharpness: {}'.format(SH_mean))\n",
    "print('#############################################################################################')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(y_test, y_test+alpha_acc_zone*y_test , (y_test-alpha_acc_zone*y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
